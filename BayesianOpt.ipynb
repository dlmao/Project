{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import one_hot\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "#from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 35000\n",
    "# Max number of words in each comment.\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "#Number of times to duplicate identity_hate\n",
    "repeats=1 \n",
    "#Size of the word embeddings\n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('spellcheckdata/spellcheck.csv')\n",
    "train = train.replace(np.nan, '', regex=True)\n",
    "\n",
    "X_train_text = train[\"comment_text\"].values\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X_train_text))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "count_list = list(tokenizer.word_counts.items())\n",
    "counts=[count_list[i][1] for i in range(0,len(count_list))]\n",
    "\n",
    "train = train.sample(frac=1,random_state=13)\n",
    "val=train.tail(int(len(train)*1/10))\n",
    "train=train.head(int(len(train)*9/10))\n",
    "\n",
    "train_idhate = train[train['identity_hate'] == 1].sample(n=500,random_state=13)\n",
    "train_threat = train[train['threat'] == 1].sample(n=300,random_state=13)\n",
    "train = pd.concat([train, train_idhate])\n",
    "train = pd.concat([train, train_threat])\n",
    "\n",
    "train = train.sample(frac=1,random_state=25)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].to_numpy()\n",
    "X_train = tokenizer.texts_to_sequences(train[\"comment_text\"].values)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_val = val[list_classes].to_numpy()\n",
    "X_val = tokenizer.texts_to_sequences(val[\"comment_text\"].values)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_function(rnn_size, dropout):\n",
    "        embedding_layer = layers.Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)        \n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(layers.SpatialDropout1D(dropout))\n",
    "        model.add(layers.Bidirectional(layers.LSTM(int(rnn_size), return_sequences=True)))\n",
    "        model.add(layers.GlobalMaxPooling1D())\n",
    "        model.add(layers.Dense(6, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy','AUC'])\n",
    "\n",
    "        \n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_val, y_val),callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, min_delta=0.0030,restore_best_weights=False)])\n",
    "\n",
    "        out=roc_auc_score(y_val, model.predict(X_val))\n",
    "        print(out)\n",
    "    \n",
    "        return out\n",
    "    \n",
    "def kernel(X1, X2, l=100, sigma_f=2.0):\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "\n",
    "def posterior(X_s, X_train, Y_train, l=100, sigma_f=2.0, sigma_y=.01):\n",
    "    K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.diag(np.ones(X_train.shape[0]))\n",
    "    K_s = kernel(X_train, X_s, l, sigma_f)\n",
    "    K_ss = kernel(X_s, X_s, l, sigma_f)+ 1e-8 * np.diag(np.ones(X_s.shape[0]))\n",
    "    K_inv = np.linalg.inv(K)\n",
    "    mu_s = np.matmul(np.matmul(np.transpose(K_s),K_inv),Y_train)\n",
    "    cov_s = K_ss - np.matmul(np.matmul(np.transpose(K_s),K_inv),K_s)\n",
    "    return ([mu_s, cov_s])\n",
    "\n",
    "def expected_improvement( x_proposed,X_train, Y_train,l=100, sigma_f=2.0, sigma_y=.01):\n",
    "    out = posterior(x_proposed, X_train, Y_train, l, sigma_f, sigma_y)\n",
    "    mu=out[0]\n",
    "    mu = mu.reshape(-1,1)\n",
    "    var=np.diag(out[1])\n",
    "    y_current=max(Y_train)\n",
    "    std = np.sqrt(var).reshape(-1,1)\n",
    "    delta = mu - y_current\n",
    "    z = np.divide(delta, std)\n",
    "    return (delta * norm.cdf(z) + std * norm.pdf(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only calculate the BO value mean/cov for a grid of points\n",
    "Xs=[]\n",
    "for i in range(0,151,2):\n",
    "    for j in range(30,150):\n",
    "        Xs.append([i,j])\n",
    "Xs=np.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Represented the dropout rates as 200*dropout rate so that we could work with integers.\n",
    "#In addition, subtracted all values by .982 to psuedo center them\n",
    "\n",
    "X_t=[[20,90],[74,90],[130,90],[74,130],[74,50]] #need to provide some initial values.\n",
    "Y_t=np.array([])\n",
    "for t in X_t:\n",
    "    rnn_size=t[1]   \n",
    "    dropout=t[0]\n",
    "    out=black_box_function(rnn_size, dropout/200)\n",
    "    Y_t=np.append(Y_t,(out-.982))\n",
    "X_t=np.array(X_t)\n",
    "for i in range(10):\n",
    "    new=Xs[np.argmax(expected_improvement(Xs, X_t, Y_t))] #choose next point to sample\n",
    "    rnn_size=new[1]\n",
    "    dropout=new[0]\n",
    "    X_t=np.vstack((X_t,[dropout,rnn_size]))  #add new x point onto the X_t array\n",
    "    out=black_box_function(rnn_size, dropout/200) #find the value for the current parameters via training a model.\n",
    "    Y_t=np.append(Y_t,(out-.982)) #add new y point onto the Y_t array\n",
    "\n",
    "Xs[np.argmax(posterior(Xs, X_t50, Y_t50)[0])] #find the maximum parameter set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
