{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the skipgram model to create word2vec encodings and then use those encodings to represent the input words to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import one_hot\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 35000\n",
    "# Max number of words in each comment.\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "#Number of times to duplicate identity_hate\n",
    "repeats=1 \n",
    "#Size of the word embeddings\n",
    "EMBED_SIZE=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the data. We are using the preprocessed data we created so we avoid redoing the processing every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('spellcheck.csv')\n",
    "train = train.replace(np.nan, '', regex=True)\n",
    "\n",
    "X_train_text = train[\"comment_text\"].values\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X_train_text))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "count_list = list(tokenizer.word_counts.items())\n",
    "counts=[count_list[i][1] for i in range(0,len(count_list))]\n",
    "\n",
    "train = train.sample(frac=1,random_state=13)\n",
    "val=train.tail(int(len(train)*1/10))\n",
    "train=train.head(int(len(train)*9/10))\n",
    "\n",
    "train_idhate = train[train['identity_hate'] == 1].sample(n=500,random_state=13)\n",
    "train_threat = train[train['threat'] == 1].sample(n=300,random_state=13)\n",
    "train = pd.concat([train, train_idhate])\n",
    "train = pd.concat([train, train_threat])\n",
    "\n",
    "train = train.sample(frac=1,random_state=25)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].to_numpy()\n",
    "X_train = tokenizer.texts_to_sequences(train[\"comment_text\"].values)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_val = val[list_classes].to_numpy()\n",
    "X_val = tokenizer.texts_to_sequences(val[\"comment_text\"].values)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "embedding_layer = layers.Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)  \n",
    "\n",
    "cnn_model = keras.Sequential()\n",
    "cnn_model.add(embedding_layer)\n",
    "cnn_model.add(layers.Conv1D(32, 3, activation='relu', padding='same'))\n",
    "cnn_model.add(layers.MaxPooling1D(2))\n",
    "cnn_model.add(layers.Conv1D(32, 3,activation='relu', padding='same'))\n",
    "cnn_model.add(layers.MaxPooling1D(2))\n",
    "cnn_model.add(layers.Conv1D(32, 3,activation='relu', padding='same'))\n",
    "cnn_model.add(layers.MaxPooling1D(2))\n",
    "cnn_model.add(layers.Conv1D(32, 3,activation='relu', padding='same'))\n",
    "cnn_model.add(layers.GlobalMaxPooling1D())\n",
    "cnn_model.add(layers.Dropout(0.1))\n",
    "cnn_model.add(layers.Dense(6, activation='sigmoid'))\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.AUC(multi_label=True)])\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_val, y_val),callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, min_delta=0.0030,restore_best_weights=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the embedding layer is still missing weights (i.e all 0's) so want to do some fine tuning to fix that. Must note that this does cause a little overfitting but it may lead to some increased model rubustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Second_epochs=1\n",
    "lr=0.000001\n",
    "opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.AUC(multi_label=True)])\n",
    "history = cnn_model.fit(X_train, y_train, epochs=Second_epochs, batch_size=batch_size,validation_split=0.1)\n",
    "\n",
    "out=roc_auc_score(y_val, model.predict(X_val))\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
