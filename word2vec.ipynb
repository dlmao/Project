{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the skipgram model to create word2vec encodings and then use those encodings to represent the input words to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import one_hot\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 35000\n",
    "# Max number of words in each comment.\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "#Number of times to duplicate identity_hate\n",
    "repeats=1 \n",
    "#Size of the word embeddings\n",
    "EMBEDDING_DIM=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('spellcheck.csv')\n",
    "train = train.replace(np.nan, '', regex=True)\n",
    "\n",
    "X_train_text = train[\"comment_text\"].values\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X_train_text))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "count_list = list(tokenizer.word_counts.items())\n",
    "counts=[count_list[i][1] for i in range(0,len(count_list))]\n",
    "\n",
    "train = train.sample(frac=1,random_state=13)\n",
    "val=train.tail(int(len(train)*1/10))\n",
    "train=train.head(int(len(train)*9/10))\n",
    "\n",
    "train_idhate = train[train['identity_hate'] == 1].sample(n=500,random_state=13)\n",
    "train_threat = train[train['threat'] == 1].sample(n=300,random_state=13)\n",
    "train = pd.concat([train, train_idhate])\n",
    "train = pd.concat([train, train_threat])\n",
    "\n",
    "train = train.sample(frac=1,random_state=25)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].to_numpy()\n",
    "X_train = tokenizer.texts_to_sequences(train[\"comment_text\"].values)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_val = val[list_classes].to_numpy()\n",
    "X_val = tokenizer.texts_to_sequences(val[\"comment_text\"].values)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec (x,X,Y,width=2):\n",
    "    for i in range(MAX_SEQUENCE_LENGTH):\n",
    "        if i<width:\n",
    "            for j in range(1,width+1):\n",
    "                new_x=x[i]\n",
    "                new_y=x[i+j]\n",
    "                if new_x and new_y:\n",
    "                    X.append(new_x)\n",
    "                    Y.append(new_y)\n",
    "            for j in range(-1,-(width+1),-1):\n",
    "                if (j+i)<0:\n",
    "                    break\n",
    "                new_x=x[i]\n",
    "                new_y=x[i+j]\n",
    "                if new_x and new_y:\n",
    "                    X.append(new_x)\n",
    "                    Y.append(new_y)\n",
    "        elif i>(MAX_SEQUENCE_LENGTH-1-width):\n",
    "            for j in range(1,width+1):\n",
    "                if (j+i)>(MAX_SEQUENCE_LENGTH-1):\n",
    "                    break\n",
    "                new_x=x[i]\n",
    "                new_y=x[i+j]\n",
    "                if new_x and new_y:\n",
    "                    X.append(new_x)\n",
    "                    Y.append(new_y)\n",
    "            for j in range(-1,-(width+1),-1):\n",
    "                new_x=x[i]\n",
    "                new_y=x[i+j]\n",
    "                if new_x and new_y:\n",
    "                    X.append(new_x)\n",
    "                    Y.append(new_y)\n",
    "        else:\n",
    "            for j in range(1,width+1):\n",
    "                new_x=x[i]\n",
    "                new_y=x[i+j]\n",
    "                if new_x and new_y:\n",
    "                    X.append(new_x)\n",
    "                    Y.append(new_y)\n",
    "            for j in range(-1,-(width+1),-1):\n",
    "                new_x=x[i]\n",
    "                new_y=x[i+j]\n",
    "                if new_x and new_y:\n",
    "                    X.append(new_x)\n",
    "                    Y.append(new_y)\n",
    "    return 0     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vX=[]\n",
    "w2vY=[]\n",
    "for i in X_train:\n",
    "    word2vec(i,w2vX,w2vY,2)\n",
    "\n",
    "w2vX=np.array(w2vX)\n",
    "w2vY=np.array(w2vY)\n",
    "w2vX=np.reshape(w2vX,(w2vX.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 256\n",
    "\n",
    "word2vec_model = keras.Sequential()\n",
    "word2vec_model.add(layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=1,mask_zero=True))\n",
    "word2vec_model.add(layers.Dense(MAX_NB_WORDS,activation='softmax'))\n",
    "\n",
    "word2vec_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "history = word2vec_model.fit(w2vX,w2vY, epochs=epochs, batch_size=batch_size,validation_split=0.05,callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2vX\n",
    "del w2vY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the word2vec embeddings, we can use those embeddings as the first layer into our LTSM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=256\n",
    "\n",
    "embedding_layer = layers.Embedding(MAX_NB_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=word2vec_model.layers[0].get_weights(),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.SpatialDropout1D(dropout))\n",
    "model.add(layers.Bidirectional(layers.LSTM(int(rnn_size), return_sequences=True)))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(6, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy','AUC'])\n",
    "\n",
    "        \n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_val, y_val),callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, min_delta=0.0030,restore_best_weights=False)])\n",
    "\n",
    "out=roc_auc_score(y_val, model.predict(X_val))\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
